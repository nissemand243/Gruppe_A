\section{Lessons learned perspective}
%\textcolor{red}{Describe the biggest issues, how you solved them, and which are major lessons learned with regards to: evolution and refactoring, operation, and maintenance of your ITU-MiniTwit systems. Link back to respective commit messages, issues, tickets, etc. to illustrate these. Also reflect and describe what was the "DevOps" style of your work. For example, what did you do differently to previous development projects and how did it work?}

In previous projects, we have not worked with an already existing code base, nor have we had to update it and implement new tools. Having to rewrite the application in a new language came with the implications of having to recreate functions with exactly the same functionality. Thus having to find dependencies to replace functionality of the python dependencies used, such as flask.


The course has highlighted that there are a wide array of different tools and solutions across programming languages. It has also made it clear that the ease of use between tools vary widely and that there is not necessarily equal support for each programming language. Additionally, we experienced that trying to replace one tool caused a ripple effect and made us look for new solutions in places that were only tangentially related to what we were trying to do in the first place. For example, one of our commit messages states: \href{https://github.com/salsitu/minitwit_thesvindler/commit/b0a6703feba3e1e3453358d563e0cce79fe8b6e3}{"The system now correctly directs to a HTML page, but the HTML pages have python dependencies"}.


This is also the first time we have had to create an API to communicate with an outside source, the simulator. Thus having to handle unknown request and adjust accordingly to errors caught by logging.


What we did differently compared to previous projects was that we took the principles of DevOps and applied them to our project. That is, we set up continuous integration, continuous deployment, implemented monitoring and logging, and scaled our system with Docker Swarm. Being aware of the DevOps process of maintaining our system also made us think about keeping our dependencies up to date. For example, we had to \href{https://github.com/salsitu/minitwit_thesvindler/commit/7785e42ec56e38445815946b7efa980dc9502044}{ update to Go version 1.18} in march.
We also had not experienced the concept of technical debt before. The course's structure of weekly releases, simulating an actual production product, showed us how fixing small difficulties and hangups can scale to a large task over time.


We used a CI/CD pipeline to continuously keep working code in our production branch while being able to quickly get new features or bug fixes merged in. Whenever we pushed new code to the production branch, the code was automatically built, tested, and deployed to the webserver. This worked well as we did not have to waste a lot of time for manually running tests and deploying the system.


The biggest issue for our group was downtime at the beginning of the simulation. The downtime meant that a significant amount of users did not get registered in our database. Thus, they did not exist for their respective API requests later in the simulation. This resulted in a lot of error HTTP responses which caused a significant fluctuation on our graph of HTTP responses. Unfortunately, it took us too long to identify why we kept on having HTTP errors where we did not expect them.


Some groups fixed this by creating users, when calls were made from users that did not already exist. 
What we wanted to do was to do a database transfer from another group who had the complete data set, but we never got to do this. If we could do it over, we would have prioritized this solution earlier, over creating the users that we did not get to register in time, since the first solution caused incomplete data sets for those groups. 
